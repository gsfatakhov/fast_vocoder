#  Vocoder poject (HW3)
## Фатахов Георгий

[HW 3](https://github.com/markovka17/dla/tree/2024/hw3_nv)

### Реализация
#### TLDR задания
Заданием было реализовать HiFiGAN вокодер. В качестве датасета использовался LJSpeech (весом 3.8Gb).

#### Описание реализации
При реализации столкнулся с проблемой нехватки памяти при локальном запуске даже при 1 батче. 
Дебаг и починка фреймов помогла решить проблему. 

Шаблон был переписан для использования с генеративной моделью. А именно изменены класс лосса и тренера.

Датасет был разделен на тестовую и валидационную выборки (95 и 5 процентов соответсвенно).

### Обучение
Обучение модели происходило на RTX 4090 (24gb) и 16 ядрах процессора.


На первом запуске обучение происходило на всем датасете, а в качестве валидации использовалась тестовая выборка. Для этого было использовано два разных даталоадера, поэтому
хоть и при небольшом batch_size использовалось много видеопамяти

https://wandb.ai/avss_proj/TTS/runs/7j9vd1ku?nw=nwusergsfatakhov

При первом большом запуске уже были выставлены все основные гиперпараметры, исползуемые в дальнейшем. Но были использованы 
метрики PESQ, которая могла сломать обучение на тихих записях и STOI, которая выдавала 1e-5 на всех аудио из-за их амплитуды.

https://wandb.ai/avss_proj/TTS/runs/u2ue388j?nw=nwusergsfatakhov

После первых запусков стало ясно, что модель упирается в CPU (из-за вычисления на нем мелспектрограмм)
Далее модель была переделана под вычисленеи мелспектрограмм на GPU. Незначительно был уменьшен батчсайз с 88 до 80, и увеличено количество воркеров с 16 до 40.
В целом данные манипуляции не сильно изменили ситуацию с утилизацией GPU.


https://wandb.ai/avss_proj/TTS/runs/u2ue388j/files/config.yaml


По ходу обучения можем видеть как лосс дискриминатора быстро падает, не давая возможности генератору обучиться,
несмотря на уменьшенный lr дискриминатора (`1e-4` -> `5e-5`).
При сбалансированном гане, лосс дискриминатора должен расти после определенного момента, когда генератор достаточно обучился и способен обманывать дискриминатор.
Также видим, впоследствии этого большие нормы градиентов (клипаются до `1000`). Это может быть также связанно с настройками lr_scheduler'ов, а именно использованием констант `0.9999` вместо `0.999` как в оригинальной статье.
(Это также может раздувать нормы градиентов)

### Аналитика

#### Внутренняя

Результаты ресинтеза 5 записей из [LJSpeech](../datasets/5_ljspeech_dataset) находятся в [папке](results/inner).

*Полученные при генерации метрики:*
```
test_PESQ      : 1.3925291299819946
test_STOI      : 0.8778601884841919
```

*Сравнение по времени и по частотам:*

![lj1.png](figures/lj1.png)
![lj2.png](figures/lj2.png)
![lj3.png](figures/lj3.png)
![lj4.png](figures/lj4.png)
![lj5.png](figures/lj5.png)

По диаграммам амплитуд мы можем заметить, что сгенерированные варианты выглядят более шумными, чем оригинальные записи.
Также можно заметить что абсолютная величина сэмплов меньше (например на первом файле в среднем `0.2` в отличии `0.4` на оргинальной записи).


По мелспектрограммам можно сказать что форманты смазаны, хотя несколько проглядываюься.
В то же время спад амплитуд в перерывах между словами сильно выражен. 


При прослушивании слышен робовойс и несильный шум, но проглядывается голос человека.
По wav форме отличить оригинал от сгенерированного сложно, а при отсутсвии второго варианта невозмонжно.
Мелспектрограммы отличаются по четкости достаточно, чтобы различать данные.


*Объяснение и выводы:*
Маленькие амплитуды можно объяснить тем, что модель "осторожничает" и не хочет сильно изменять амплитуду, чтобы не нарушить звук.
Дальнейшее окрашиваение звука, будет добавлять амплитуды и тем самым придавать четкость мелспектрограммам.
Можно сделать вывод, что генератор модели еще можно дообучить.

#### Внешняя

Результаты ресинтеза 5 записей из [Внешнего датасета](../datasets/vocoder_dataset) находятся в [папке](results/outer).

*Полученные при генерации метрики:*
```
test_PESQ      : 1.242226243019104
test_STOI      : 0.85208500623703
```
Чуть меньше чем при внутреннем датасете. 

*Сравнение по времени и по частотам:*

![res_1.png](figures/res_1.png)
![res_2.png](figures/res_2.png)
![res_3.png](figures/res_3.png)
![res_4.png](figures/res_4.png)
![res_5.png](figures/res_5.png)

В данном случае уменьшение амплитуды не так сильно выражено, как в предыдущем случае.
Хотя наличие шума очевидно.


На примере `pipin.waw` можем видеть как расплывается мелспектрограмма.

Также слышен робовойс, слова понятны. Также слышен голос автора.

*Объяснение и выводы:*

В итоге, можно сказать что результат аналогичен внутреннему датасету, но с небольшими отличиями (в виде метрик и амплитуд).


#### Full-TTS система

##### Внутренняя

Результаты синтеза 5 записей из [LJSpeech](../datasets/tts_ljspeech_dataset) находятся в [папке](results/full_tts/inner).

*Сравнение по времени и по частотам:*

![syn_inner_1.png](figures/syn_inner_1.png)
![syn_inner_2.png](figures/syn_inner_2.png)
![syn_inner_3.png](figures/syn_inner_3.png)
![syn_inner_4.png](figures/syn_inner_4.png)
![syn_inner_5.png](figures/syn_inner_5.png)

Как мы можем видеть некоторые средние значения в wav формах были раздуты. Присуствует небольшой шум, между словами.

Мелспектрограммы стали еще более размытыми. Теперь очень сложно заметить форманты голоса.

Сильный робовойс.

*Объяснение и выводы:*

Как мы можем видеть модель была лучше обучена на gt мелспектрограммах. На сгенерированных, она очевидно дает худшую картину.

##### Внешняя

Результаты синтеза 5 записей из [внешнего датасета](../datasets/tts_dataset) находятся в [папке](results/full_tts/outer).

*Сравнение по времени и по частотам:*

![syn_outer_1.png](figures/syn_outer_1.png)
![syn_outer_2.png](figures/syn_outer_2.png)
![syn_outer_3.png](figures/syn_outer_3.png)
![syn_outer_4.png](figures/syn_outer_4.png)
![syn_outer_5.png](figures/syn_outer_5.png)

Как видим в wav амплитуды не уменьшены.

Мелспектрограммы похожи на вариант без TTS.

Робовойс.

*Объяснение и выводы:*

Четкость мелспек сопоставима с вариантом без TTS, т.к. для модели это все одинаковые внешние данные. Но робовойс слышен отчетливее.


##### Выводы по tts

Робовойс слышен отчетливее из-за того что спектрограммы также неидеальны. Разницы между внешним и внутреним датасетами в плане слышимости голоса практически нет.
Это опять же объясняется новизной для модели данных.

### Оценка результатов

Результаты подлежащие оценке, находятся в [папке](results/outer).
То есть это результаты ресинтеза на внещнем датасете.

Судя шкале из оценивания, я бы оценил полученный результат как:
```
4. There is some obvious noise, but all the words are clear and you do not have to listen to them to understand
```
Т.к. ничего не сказано про робовойс.


Подсчет при помощи [WVMOS](https://github.com/AndreevP/wvmos):

```
Average mos: -1.08

Результаты по отдельным файлам:
File: shostakovich.wav - MOS: -1.00
File: theremin.wav - MOS: -0.08
File: dla.wav - MOS: -2.29  # (из-за sr=44100)
File: pupin.wav - MOS: -1.33
File: bernstein.wav - MOS: -0.70
```


