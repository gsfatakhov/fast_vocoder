#  Vocoder poject (HW3)
## Фатахов Георгий

### Реализация
Заданием было реализовать HiFiGAN в качестве вокодера. В качестве датасета использовался LJSpeech (веосм 3.8Gb).
Я приступил к задаче реализации референсной модели. Изначально сталиквался с проблемой нехватки памяти при локальном запуске даже при 1 батче. 
Дебаг и починка фреймов помогла решить проблему. 

Шаблон был переписан для использования с генеративной моделью. А именно изменены класс лосса и тренера.

Датасет был разделен на тестовую и валидационную выборки (95 и 5 процентов соответсвенно).




### Обучение
Обучение модели происходило на RTX 4090 (24gb) и 16 ядрах процессора.



На первом запуске обучение происходило на всем датасете, а в качестве валидации использовалась тестовая выборка. Для этого было использовано два разных даталоадера, поэтому
хоть и при небольшом batch_size использовалось много видеопамяти

https://wandb.ai/avss_proj/TTS/runs/7j9vd1ku?nw=nwusergsfatakhov

При первом большом запуске уже были выставлены все основные гиперпараметры, исползуемые в дальнейшем. Но были использованы 
метрики PESQ, которая могла сломать обучение на тихих записях и STOI, которая выдавала 1e-5 на всех записях из-за их амплитуды.

https://wandb.ai/avss_proj/TTS/runs/u2ue388j?nw=nwusergsfatakhov

После первых запусков стало ясно, что модель упирается в CPU (из-за вычисления на нем мелспектрограмм)
Далее модель была переделана под вычисленеи мелспектрограмм на GPU. Незначительно был уменьшен батчсайз с 88 до 80, и увеличено количество воркеров с 16 до 40.
В целом данные манипуляции не сильно изменили ситуацию с утилизацией GPU.


https://wandb.ai/avss_proj/TTS/runs/u2ue388j/files/config.yaml


По ходу обучения можем видеть как лосс дискриминатора быстро падает, не давая возможности генератору обучиться,
несмотря на уменьшенный lr дискриминатора (`1e-4` -> `5e-5`).
При сбалансированном гане, лосс дискриминатора должен расти после определенного момента, когда генератор достаточно обучился и способен обманывать дискриминатор.
Также видим, впоследствии этого большие нормы градиентов (клипаются до `1000`). Это может быть также связанно с настройками lr_scheduler'ов, а именно использованием констант `0.9999` вместо `0.999` как в оригинальной статье.
(Это также может раздувать нормы градиентов)




### Результаты


Для установки зависимострей
```bash
pip install omegaconf==2.0.0
pip install fairseq==0.12.2 --no-deps
pip install -r requirements.txt
pip install bitarray sacrebleu 
pip install hydra-core==1.0.7 omegaconf==2.0.6 Cython
```