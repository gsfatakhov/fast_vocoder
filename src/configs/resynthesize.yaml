defaults:
  - model: lightvoc
  - metrics: test
  - transforms: example_only_batch
  - _self_
dataloader:
  _target_: torch.utils.data.DataLoader
  batch_size: 1
  num_workers: 8
  pin_memory: True
datasets:
  test:
    _target_: src.datasets.LJSpeechDataset
    name: "test"
    audio_path: "datasets/vocoder_dataset"
    index_audio_path: ${datasets.test.audio_path}
    segment_length: null
    mel_config:
      _target_: src.utils.MelSpectrogramConfig
      sr: 22050
      win_length: 1024
      hop_length: 256
      n_fft: 1024
      f_min: 0
      f_max: 8000
      n_mels: 80
      power: 1.0
      pad_value: -11.5129251
inferencer:
  device_tensors: ["audio", "mel"] # which tensors should be on device (ex. GPU)
  device: auto # device name or "auto"
  save_path: "resynthesize_results" # any name here, can be a dataset name
  seed: 7
  from_pretrained: "/Users/georgijfatahov/Downloads/lightvoc_176bs_server/model_best.pth" # path to the pretrained model
